---
title: "STAT387 Project 5"
author: ""
output: rmdformats::material
---

# Project 5


```{r}
library(dplyr)
library(ggplot2)
library(glmnet)
library(class)
library(e1071)
library(boot)
library(MASS)
library(caTools)
library(leaps)

germancredit <- read.csv("/Users/ht/Desktop/STAT387/Project/germancredit.csv", header = TRUE)
germancredit <- na.omit(germancredit)
germancredit
```

**a) Perform an exploratory analysis of data**


```{r}
summary(germancredit)

```


The data set contains information about the credit information of 1000 people in Germany and whether they are defaulted on their credit or not.The data consists of 21 variables, most of which are categorical data including the response Default. The response Default falls into one of two categories, 1 means defaulted and 0 means not defaulted.
The continuous variables are: duration, amount, installment, residence, age, cards, liable
The categorical variables are: Default, checkingstatus1, history, purpose, savings, employ, status, others, property, otherplans, housing, job, tele, foreign.




```{r}
cor(germancredit[, sapply(germancredit, is.numeric)])

```
The correlation matrix provide information about the linear relationship between the variables in the data.
- The duration, amount, installment and residence all have positive relationship with the response Default, meaning that as these variables increase the likelihood default also increase. The correlation coefficient values of duration and amount indicate a slightly weak relationship, while the variable installment and residence have a very weak positive relationship with Default.
- There is a negative relationship between Default and age, cards and liable. The higher the age, cards and liable of the credit users, the more likely that their default chance will reduce. The correlation coefficients also indicate that this negative relationship is weak.





**b) Build a reasonably “good” logistic regression model for these data. There is no need to explore interactions. Carefully justify all the choices you make in building the model **


```{r}
set.seed(123)
data_split <- sample.split(germancredit, SplitRatio = 0.8)
train <- subset(germancredit, data_split == TRUE)
test <- subset(germancredit, data_split == FALSE)

```


```{r}
# stepwise AIC selection method
s_aic <- stepAIC(glm(Default ~ ., data = train, family = binomial), direction = "both")
summary(s_aic)

```

According to the stepwise AIC selection method, the best model with the smallest AIC includes the following predictors: checkingstatus1, duration, history, purpose, amount, savings, employ, installment, others, age, otherplans, tele and foreign. 


```{r}
# Subset selection method
regfit <- regsubsets(Default ~ ., data = train, really.big = TRUE)
which.min(summary(regfit)$bic)
coef(regfit,8)

```

Using the subset selection method, the model with the lowest BIC statistics contains 7 predictors: checkingstatus1, duration, history, purpose, savings, others and otherplans. This is the best model according to this method. 

So now we have 2 models to compare, call them glm1 and glm2. Lets look at the confusion matrix and see how the 2 models compare.

```{r}
# Fit logistic model 1
glm1 <- glm(Default ~ checkingstatus1 + duration + history + purpose + amount + savings + employ + installment + others + age + otherplans + tele + foreign, family = binomial, data = train)

# Confusion matric for model 1
pred_glm1 <- predict(glm1, newdata = test, type = "response")
pred_glm1 <- ifelse(pred_glm1 > 0.5, 1, 0)
table(pred_glm1, test$Default)

# Percentage of correct prediction of model 1
mean(pred_glm1 == test$Default)
```


```{r}
# Fit logistic model 2
glm2 <- glm(Default ~ checkingstatus1 + duration + history + purpose + savings + others + otherplans, family = binomial, data = train)

# Confusion matrix for model 2
pred_glm2 <- predict(glm2, newdata = test, type = "response")
pred_glm2 <- ifelse(pred_glm2 > 0.5, 1, 0)
table(pred_glm2, test$Default)

# Percentage of correct prediction of model 2
mean(pred_glm2 == test$Default)
```

From the confusion matrix, we can calculate the percentage of correctly predicted defaulted credit. Since model 1 has higher accuracy (74.35%) compared to model 2(72.69%), it appears that model 1 is better in terms of accuracy on the test data. Therefore, glm1 can be considered the better model among these two. 

The final model will contains 13 predictors: checkingstatus1, duration, history, purpose, amount, savings, employ, installment, others, age, otherplans, tele and foreign. 

**c) Write the final model in equation form. Provide a summary of estimates of the regression coefficients, the standard errors of the estimates, and 95% confidence intervals of the coefficients. Interpret the estimated coefficients of at least two predictors. Provide training error rate for the model.**


```{r}

```



**d) Fit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.**


```{r}

```



**e) Repeat (d) using LDA**


```{r}

```



**f) Repeat (d) using QDA**


```{r}

```

**g) Compare the results in (b), (d)-(f). Which classifier would you recommend? Justify your answer.**


```{r}

```
